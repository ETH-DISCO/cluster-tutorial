# Using Conda

> [!CAUTION]
> Unfortunately this workflow is painfully slow in practice. â€“ I believe that this is because NFS4 is sensitive to handling a large quantity of files, like those generated by Conda. This could explain why some file-related commands take up to an hour to run, while heavy computational tasks using CPU/GPU resources complete quickly. Fortunately compute nodes use the EXT4 filesystem which is significantly faster.
> 
> Fortunately, our admins provide Apptainer, a containerization tool similar to Docker but designed for HPC environments. I recommend using Apptainer over Conda and moving all dependencies to the compute-nodes.

By running Slurm scripts you won't see the logs in real-time which can slow you down during development. For debugging or prototyping purposes it might make sense to attach your terminal to individual compute nodes and then execute scripts yourself. A jupyter notebook can also be convenient for this use-case:

```bash
conda clean --all

# check node availability
grep --color=always --extended-regexp 'free|$' /home/sladmitet/smon.txt

# attach to a tikgpu06 node assuming it's free
srun --mem=50GB --gres=gpu:01 --nodelist tikgpu06 --pty bash -i

# set up storage
mkdir -p /scratch/$USER
cd /scratch/$USER

# run notebook
conda create --name jupyternb notebook --channel conda-forge
conda activate jupyternb
jupyter notebook --no-browser --port 5998 --ip $(hostname -f) # port range [5900-5999]
```

The last instruction will display a public link that you can then use to access the notebook.

To add additional dependencies once you're in the conda environment you will need to the following flags:

```bash
pip install <dependency> --upgrade --no-cache-dir --user --verbose

echo 'export PATH="$HOME/.local/bin:$PATH"' >> ~/.bashrc
source ~/.bashrc
```

Make sure to deactivate the Conda environment when you're done:

```bash
conda deactivate
conda env list
conda remove --all --yes --name jupyternb
```

